[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "Homework5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\nDescribe the bagged tree algorithm.\n\n\nThe bagged tree algorithm is a method in which we attempt to find accuracy within trees. The bagged tree algorithm takes in multiple fitted trees and aggregates them over a designated response variable in an attempt to find a prediction that holds for the accumulated trees.\n\n\nWhat is meant by a general linear model?\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\nWhy do we split our data into a training and test set?"
  },
  {
    "objectID": "Homework5.html#task-1-conceptual-questions",
    "href": "Homework5.html#task-1-conceptual-questions",
    "title": "Homework5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\nDescribe the bagged tree algorithm.\n\n\nThe bagged tree algorithm is a method in which we attempt to find accuracy within trees. The bagged tree algorithm takes in multiple fitted trees and aggregates them over a designated response variable in an attempt to find a prediction that holds for the accumulated trees.\n\n\nWhat is meant by a general linear model?\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\nWhy do we split our data into a training and test set?"
  },
  {
    "objectID": "Homework5.html#task-2-fitting-models",
    "href": "Homework5.html#task-2-fitting-models",
    "title": "Homework5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\n\nQuick EDA/Data Preparation\n\nQuickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(randomForest)\n\n\noriginalData&lt;-read.csv(\"https://www4.stat.ncsu.edu/~online/datasets/heart.csv\")\n\n# If RestingBP or Cholesterol = 0 then patient would be dead thus we can assume 0 is a missing/Null value\n# Replaces ) with NA\noriginalData$RestingBP&lt;-na_if(originalData$RestingBP,0)\noriginalData$Cholesterol&lt;-na_if(originalData$Cholesterol,0)\n\n# Find number of NA rows\nsum(is.na(originalData))\n\n[1] 173\n\n# Create new dataframe with no na rows\nheartDisease&lt;-originalData|&gt;\n  drop_na()\n\n# Create Summary table with respect to heart disease\nsummary(heartDisease)\n\n      Age            Sex            ChestPainType        RestingBP  \n Min.   :28.00   Length:746         Length:746         Min.   : 92  \n 1st Qu.:46.00   Class :character   Class :character   1st Qu.:120  \n Median :54.00   Mode  :character   Mode  :character   Median :130  \n Mean   :52.88                                         Mean   :133  \n 3rd Qu.:59.00                                         3rd Qu.:140  \n Max.   :77.00                                         Max.   :200  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   : 85.0   Min.   :0.0000   Length:746         Min.   : 69.0  \n 1st Qu.:207.2   1st Qu.:0.0000   Class :character   1st Qu.:122.0  \n Median :237.0   Median :0.0000   Mode  :character   Median :140.0  \n Mean   :244.6   Mean   :0.1676                      Mean   :140.2  \n 3rd Qu.:275.0   3rd Qu.:0.0000                      3rd Qu.:160.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:746         Min.   :-0.1000   Length:746         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.5000   Mode  :character   Median :0.0000  \n                    Mean   : 0.9016                      Mean   :0.4772  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n\n\nCreate a new variable that is a factor version of the HeartDisease variable (if needed, this depends on how you read in your data). Remove the ST_Slope variable and the original HeartDisease variable (if applicable).\n\n\n# Change HeartDisease to factor, drop ST_Slope\nheartDisease&lt;-heartDisease|&gt;\n  select(-ST_Slope)\n\n\nWe’ll be doing a kNN model below to predict whether or not someone has heart disease. To use kNN we generally want to have all numeric predictors (although we could try to create our own loss function as an alternative). In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina ChestPainType, and RestingECG.\n\nCreate dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\n# Use dummyVars to transform all categorical variables to numerics.\n# Adds each unique text entry as its own variable where 0=False and 1=True.\ndummies&lt;-dummyVars(~.,data = heartDisease, sep = \".\")\n\nheartDisease2&lt;-predict(dummies,newdata = heartDisease)|&gt;\n  as_tibble()|&gt;\n  mutate(HeartDisease=as.factor(HeartDisease))\n\n\n\nSplit your Data\nsplit your data into a training and test set. (ideally you’d do this prior to the EDA so that info from the EDA doesn’t bias what you do modeling-wise, but that isn’t usually done.)\n\n# Set the seed to enable reproducible \nset.seed(8)\n# Create a numeric vector that will be used to select rows in heartDisease_2 for split\ntrainingVector&lt;- sample(1:nrow(heartDisease2),size = nrow(heartDisease2)*.8)\n\n# Subset heartDisease_2 into  training and test data sets\nheartTrain&lt;-heartDisease2[trainingVector,]\n\nheartTest&lt;-heartDisease2[-trainingVector,]\n\n\n\nkNN\nNext, we’ll fit a kNN model. The article here gives a great example of selecting the number of neighbors to use with the caret package.\nYou don’t have to use all the variables from your dataset when fitting the model. However, you should only use numeric variables.\nThey use repeated 10 fold cross-validation. Although computationally intensive, doing repeated CV helps to give a more stable prediction of CV error. This is similar to how a mean is less variable than a single value. Since there is some inherent randomness in doing a CV computation, we can get an overall more stable result by averaging a few runs of the CV algorithm!\nTrain the kNN model. Use repeated 10 fold cross-validation, with the number of repeats being 3. You should also preprocess the data by centering and scaling. When fitting the model, set the tuneGrid so that you are considering values of k of 1, 2, 3, . . . , 40. (Note: From the help for the train() function it says: tuneGrid A data frame with possible tuning values. The columns are named the same as the tuning parameters. The name of the tuning parameter here is k.) Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\ntrctrl&lt;-trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nknn_fit&lt;-train(as.factor(HeartDisease) ~.,\n               data = heartTrain,\n               method = \"knn\",\n               trControl=trctrl,\n               preProcess=c(\"center\",\"scale\"),\n               tuneLength=10)\n\nknn_fit\n\nk-Nearest Neighbors \n\n596 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 536, 537, 536, 537, 537, 536, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   5  0.7935499  0.5843494\n   7  0.7980038  0.5931460\n   9  0.8086723  0.6146146\n  11  0.8097740  0.6165945\n  13  0.8064595  0.6096593\n  15  0.8064313  0.6098513\n  17  0.8131733  0.6233438\n  19  0.8091996  0.6154420\n  21  0.8108663  0.6186960\n  23  0.8120151  0.6211902\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 17.\n\nplot(knn_fit)\n\n\n\n\n\n\n\ntestPred&lt;-predict(knn_fit,newdata = heartTest)\ntestPred\n\n  [1] 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0\n [38] 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1\n [75] 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1\n[112] 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0\n[149] 1 0\nLevels: 0 1\n\nconfusionMatrix(testPred,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 67 20\n         1  7 56\n                                         \n               Accuracy : 0.82           \n                 95% CI : (0.749, 0.8779)\n    No Information Rate : 0.5067         \n    P-Value [Acc &gt; NIR] : 1.379e-15      \n                                         \n                  Kappa : 0.6408         \n                                         \n Mcnemar's Test P-Value : 0.02092        \n                                         \n            Sensitivity : 0.9054         \n            Specificity : 0.7368         \n         Pos Pred Value : 0.7701         \n         Neg Pred Value : 0.8889         \n             Prevalence : 0.4933         \n         Detection Rate : 0.4467         \n   Detection Prevalence : 0.5800         \n      Balanced Accuracy : 0.8211         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n\nLogistic Regression\nUsing your EDA, posit three different logistic regression models. Note: You don’t have to use the dummy columns you made here as the glm() function (and the caret implementation of it) can handle factor/ character variables as predictors.\nFit those models on the training set, using repeated CV as done above. You can preprocess the data or not, up to you.\n\n# ageHeart_train&lt;- heartDisease[trainingVector,]\n# ageHeart_test&lt;- heartDisease[-trainingVector,]\n# ageHeart_test$HeartDisease&lt;-as.factor(ageHeart_test$HeartDisease)\n\n# ageHeart_fit&lt;-train(as.factor(HeartDisease) ~.,\n#                data = ageHeart_train,\n#                method = \"glm\",\n#                trControl=trctrl,\n#                preProcess=c(\"center\",\"scale\"),\n#                tuneLength=10)\n# ageHeart_fit\nage_glmFit&lt;-train(as.factor(HeartDisease)~Age,\n                  data=heartTrain,\n                  method=\"glm\",\n                  trControl = trctrl,\n                  preProcess=c(\"center\",\"scale\"),\n                  tuneLength=10,\n                  family=\"binomial\")\nage_glmFit\n\nGeneralized Linear Model \n\n596 samples\n  1 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (1), scaled (1) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 536, 536, 537, 536, 536, 536, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6538795  0.3044333\n\nsummary(age_glmFit)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.13954    0.08693  -1.605    0.108    \nAge          0.71953    0.09541   7.542 4.63e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 824.06  on 595  degrees of freedom\nResidual deviance: 758.23  on 594  degrees of freedom\nAIC: 762.23\n\nNumber of Fisher Scoring iterations: 4\n\nheartTrain$HeartDisease&lt;-as.numeric(as.character(heartTrain$HeartDisease))\n\nageHeart_sum&lt;-heartTrain|&gt;\n  group_by(Age)|&gt;\n  summarise(probHeartDisease = mean(HeartDisease), n=n())\n\nggplot(ageHeart_sum,aes(x=Age,y=probHeartDisease))+\n  geom_point(stat = \"identity\",aes(size = n))+\n  stat_smooth(data = heartTrain,\n              aes(x=Age, y=HeartDisease),\n              method = \"glm\",\n              method.args = list(family=\n                                   \"binomial\"))+\n  theme(axis.text.x = element_text(angle=45,\n                                   vjust = 1,\n                                   hjust = 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nageTestPred&lt;-predict(age_glmFit,\n                     newdata = heartTest,\n                     type = \"raw\")\nageTestPred\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n [38] 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n [75] 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0\n[112] 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n[149] 0 1\nLevels: 0 1\n\nconfusionMatrix(ageTestPred,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 55 37\n         1 19 39\n                                         \n               Accuracy : 0.6267         \n                 95% CI : (0.544, 0.7042)\n    No Information Rate : 0.5067         \n    P-Value [Acc &gt; NIR] : 0.002038       \n                                         \n                  Kappa : 0.2556         \n                                         \n Mcnemar's Test P-Value : 0.023103       \n                                         \n            Sensitivity : 0.7432         \n            Specificity : 0.5132         \n         Pos Pred Value : 0.5978         \n         Neg Pred Value : 0.6724         \n             Prevalence : 0.4933         \n         Detection Rate : 0.3667         \n   Detection Prevalence : 0.6133         \n      Balanced Accuracy : 0.6282         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\n# cholHeart_train&lt;- heartDisease[trainingVector,]\n# cholHeart_test&lt;- heartDisease[-trainingVector,]\n# cholHeart_test$HeartDisease&lt;-as.factor(cholHeart_test$HeartDisease)\n\nchol_glmFit&lt;-train(as.factor(HeartDisease) ~ Cholesterol,\n                data = heartTrain,\n                method = \"glm\",\n                trControl=trctrl,\n                preProcess=c(\"center\",\"scale\"),\n                tuneLength=10,\n                family=\"binomial\")\n\n# cholHeart_fit\n#chol_glmFit&lt;-glm(HeartDisease~Cholesterol,data=heartTrain,family=\"binomial\")\nchol_glmFit\n\nGeneralized Linear Model \n\n596 samples\n  1 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (1), scaled (1) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 536, 536, 537, 536, 536, 536, ... \nResampling results:\n\n  Accuracy   Kappa     \n  0.5295857  0.02091478\n\nsummary(chol_glmFit)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.12129    0.08234  -1.473   0.1407  \nCholesterol  0.16219    0.08364   1.939   0.0525 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 824.06  on 595  degrees of freedom\nResidual deviance: 820.22  on 594  degrees of freedom\nAIC: 824.22\n\nNumber of Fisher Scoring iterations: 4\n\n#heartDisease$HeartDisease&lt;-as.numeric(as.character(heartDisease$HeartDisease))\ncholHeart_sum&lt;-heartTrain|&gt;\n  group_by(Cholesterol)|&gt;\n  summarise(probHeartDisease = mean(HeartDisease), n=n())\n\n\n\nggplot(cholHeart_sum,aes(x=Cholesterol,y=probHeartDisease),size=n)+\n  geom_point(stat = \"identity\",aes(size = n))+\n  stat_smooth(data = heartTrain,\n              aes(x=Cholesterol, y=HeartDisease),\n              method = \"glm\",\n              method.args = list(family=\n                                   \"binomial\"))+\n  theme(axis.text.x = element_text(angle=45,\n                                   vjust = 1,\n                                   hjust = 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncholTestPred&lt;-predict(chol_glmFit,\n                     newdata = heartTest,\n                     type = \"raw\")\ncholTestPred\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n[149] 0 0\nLevels: 0 1\n\nconfusionMatrix(cholTestPred,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 63 59\n         1 11 17\n                                          \n               Accuracy : 0.5333          \n                 95% CI : (0.4502, 0.6151)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.284           \n                                          \n                  Kappa : 0.0744          \n                                          \n Mcnemar's Test P-Value : 1.937e-08       \n                                          \n            Sensitivity : 0.8514          \n            Specificity : 0.2237          \n         Pos Pred Value : 0.5164          \n         Neg Pred Value : 0.6071          \n             Prevalence : 0.4933          \n         Detection Rate : 0.4200          \n   Detection Prevalence : 0.8133          \n      Balanced Accuracy : 0.5375          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n# maxHRHeart_train&lt;- heartDisease[trainingVector,]\n# maxHRHeart_test&lt;- heartDisease[-trainingVector,]\n# maxHRHeart_test$HeartDisease&lt;-as.factor(cholHeart_test$HeartDisease)\n\n#Cross Validation\nmaxHR_glmFit&lt;-train(as.factor(HeartDisease) ~MaxHR,\n               data = heartTrain,\n               method = \"glm\",\n               trControl=trctrl,\n               preProcess=c(\"center\",\"scale\"),\n               tuneLength=10,\n               family=\"binomial\")\n# maxHRHeart_fit\n#maxHR_glmFit&lt;-glm(HeartDisease~MaxHR,data=heartTrain,family=\"binomial\")\nmaxHR_glmFit\n\nGeneralized Linear Model \n\n596 samples\n  1 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (1), scaled (1) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 537, 536, 537, 536, 536, 536, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6795104  0.3546349\n\nsummary(maxHR_glmFit)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.13911    0.08984  -1.548    0.122    \nMaxHR       -0.93539    0.10175  -9.193   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 824.06  on 595  degrees of freedom\nResidual deviance: 719.84  on 594  degrees of freedom\nAIC: 723.84\n\nNumber of Fisher Scoring iterations: 4\n\n#heartDisease$HeartDisease&lt;-as.numeric(as.character(heartDisease$HeartDisease))\nmaxHRHeart_sum&lt;-heartTrain|&gt;\n  group_by(MaxHR)|&gt;\n  summarise(probHeartDisease = mean(HeartDisease), n=n())\n\n\n\nggplot(maxHRHeart_sum,aes(x=MaxHR,y=probHeartDisease),size=n)+\n  geom_point(stat = \"identity\",aes(size = n))+\n  stat_smooth(data = heartTrain,\n              aes(x=MaxHR, y=HeartDisease),\n              method = \"glm\",\n              method.args = list(family=\n                                   \"binomial\"))+\n  theme(axis.text.x = element_text(angle=45,\n                                   vjust = 1,\n                                   hjust = 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmaxHRTestPred&lt;-predict(maxHR_glmFit,\n                       newdata = heartTest,\n                       type = \"raw\")\nmaxHRTestPred\n\n  [1] 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n [38] 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1\n [75] 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n[112] 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n[149] 0 0\nLevels: 0 1\n\nconfusionMatrix(maxHRTestPred,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 61 40\n         1 13 36\n                                          \n               Accuracy : 0.6467          \n                 95% CI : (0.5645, 0.7229)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.0003740       \n                                          \n                  Kappa : 0.2966          \n                                          \n Mcnemar's Test P-Value : 0.0003551       \n                                          \n            Sensitivity : 0.8243          \n            Specificity : 0.4737          \n         Pos Pred Value : 0.6040          \n         Neg Pred Value : 0.7347          \n             Prevalence : 0.4933          \n         Detection Rate : 0.4067          \n   Detection Prevalence : 0.6733          \n      Balanced Accuracy : 0.6490          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nIdentify your best model and provide a basic summary of it.\n\nThe best model is the Max Heart Rate by Probability of Heart Disease model.\n\n\nsummary(maxHR_glmFit)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.13911    0.08984  -1.548    0.122    \nMaxHR       -0.93539    0.10175  -9.193   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 824.06  on 595  degrees of freedom\nResidual deviance: 719.84  on 594  degrees of freedom\nAIC: 723.84\n\nNumber of Fisher Scoring iterations: 4\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\nconfusionMatrix(maxHRTestPred,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 61 40\n         1 13 36\n                                          \n               Accuracy : 0.6467          \n                 95% CI : (0.5645, 0.7229)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.0003740       \n                                          \n                  Kappa : 0.2966          \n                                          \n Mcnemar's Test P-Value : 0.0003551       \n                                          \n            Sensitivity : 0.8243          \n            Specificity : 0.4737          \n         Pos Pred Value : 0.6040          \n         Neg Pred Value : 0.7347          \n             Prevalence : 0.4933          \n         Detection Rate : 0.4067          \n   Detection Prevalence : 0.6733          \n      Balanced Accuracy : 0.6490          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nTree Models\nIn this section we’ll fit a few different tree based models in a similar way as above!\nChoose your own variables of interest (as with logistic regression, this models can accept factor/character variables as predictors). Use repeated 10 fold CV to select a best\n\nclassification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002, . . . , 0.1)\n\n\n#maxHR,RestingHR,cholesteral\nmaxHRFit&lt;-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,\n               data = heartTrain,\n               method = \"rpart\",\n               preProcess=c(\"center\",\"scale\"),\n               trControl=trctrl,\n               tuneGrid=data.frame(cp=c(seq(0,.1, by=0.001)\n                                        )\n                                   )\n               )\nprint(maxHRFit)\n\nCART \n\n596 samples\n  3 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 536, 537, 536, 536, 536, 537, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.6039360  0.2046919\n  0.001  0.6061676  0.2093446\n  0.002  0.6240866  0.2457839\n  0.003  0.6240960  0.2452436\n  0.004  0.6348305  0.2673806\n  0.005  0.6409605  0.2797412\n  0.006  0.6465443  0.2917986\n  0.007  0.6505273  0.3000000\n  0.008  0.6561299  0.3111613\n  0.009  0.6583898  0.3154875\n  0.010  0.6617137  0.3221988\n  0.011  0.6611582  0.3209442\n  0.012  0.6617420  0.3214266\n  0.013  0.6617420  0.3214266\n  0.014  0.6622787  0.3218070\n  0.015  0.6622787  0.3215441\n  0.016  0.6633992  0.3238631\n  0.017  0.6633992  0.3238631\n  0.018  0.6639548  0.3243338\n  0.019  0.6639548  0.3243338\n  0.020  0.6678908  0.3312160\n  0.021  0.6678908  0.3312160\n  0.022  0.6683804  0.3319844\n  0.023  0.6683804  0.3319844\n  0.024  0.6706215  0.3359371\n  0.025  0.6706215  0.3359371\n  0.026  0.6706215  0.3359371\n  0.027  0.6706215  0.3359371\n  0.028  0.6706215  0.3359371\n  0.029  0.6706215  0.3359371\n  0.030  0.6695104  0.3344170\n  0.031  0.6695104  0.3344170\n  0.032  0.6644915  0.3250589\n  0.033  0.6644915  0.3250589\n  0.034  0.6644915  0.3250589\n  0.035  0.6644915  0.3250589\n  0.036  0.6565819  0.3103343\n  0.037  0.6565819  0.3103343\n  0.038  0.6565819  0.3103343\n  0.039  0.6565819  0.3103343\n  0.040  0.6543220  0.3064012\n  0.041  0.6543220  0.3064012\n  0.042  0.6526554  0.3040805\n  0.043  0.6526554  0.3040805\n  0.044  0.6526554  0.3040805\n  0.045  0.6526554  0.3040805\n  0.046  0.6526554  0.3040805\n  0.047  0.6526554  0.3040805\n  0.048  0.6526554  0.3040805\n  0.049  0.6526554  0.3040805\n  0.050  0.6526554  0.3040805\n  0.051  0.6526554  0.3040805\n  0.052  0.6526554  0.3040805\n  0.053  0.6526554  0.3040805\n  0.054  0.6526554  0.3040805\n  0.055  0.6526554  0.3040805\n  0.056  0.6526554  0.3040805\n  0.057  0.6526554  0.3040805\n  0.058  0.6526554  0.3040805\n  0.059  0.6526554  0.3040805\n  0.060  0.6526554  0.3040805\n  0.061  0.6526554  0.3040805\n  0.062  0.6526554  0.3040805\n  0.063  0.6526554  0.3040805\n  0.064  0.6526554  0.3040805\n  0.065  0.6526554  0.3040805\n  0.066  0.6526554  0.3040805\n  0.067  0.6526554  0.3040805\n  0.068  0.6526554  0.3040805\n  0.069  0.6526554  0.3040805\n  0.070  0.6526554  0.3040805\n  0.071  0.6526554  0.3040805\n  0.072  0.6526554  0.3040805\n  0.073  0.6526554  0.3040805\n  0.074  0.6526554  0.3040805\n  0.075  0.6526554  0.3040805\n  0.076  0.6526554  0.3040805\n  0.077  0.6526554  0.3040805\n  0.078  0.6526554  0.3040805\n  0.079  0.6526554  0.3040805\n  0.080  0.6526554  0.3040805\n  0.081  0.6526554  0.3040805\n  0.082  0.6526554  0.3040805\n  0.083  0.6526554  0.3040805\n  0.084  0.6526554  0.3040805\n  0.085  0.6526554  0.3040805\n  0.086  0.6526554  0.3040805\n  0.087  0.6526554  0.3040805\n  0.088  0.6526554  0.3040805\n  0.089  0.6526554  0.3040805\n  0.090  0.6526554  0.3040805\n  0.091  0.6526554  0.3040805\n  0.092  0.6526554  0.3040805\n  0.093  0.6526554  0.3040805\n  0.094  0.6526554  0.3040805\n  0.095  0.6526554  0.3040805\n  0.096  0.6526554  0.3040805\n  0.097  0.6526554  0.3040805\n  0.098  0.6526554  0.3040805\n  0.099  0.6526554  0.3040805\n  0.100  0.6526554  0.3040805\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.029.\n\nplot(maxHRFit)\n\n\n\n\n\n\n\ntreePredict&lt;-predict(maxHRFit,\n        newdata = heartTest,\n        type=\"raw\"\n        )\n\n\na random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , # of predictors (bagging is a special case here!)\n\n\n# Creates a random forrest to find out how many predictors should be used\nForrestFit&lt;-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,\n               data = heartTrain,\n               method = \"rf\",\n               preProcess=c(\"center\",\"scale\"),\n               trControl=trctrl,\n               tuneGrid=data.frame(mtry=1)\n               )\n\nforrestPredict&lt;-predict(ForrestFit,\n        newdata = heartTest,\n        type=\"raw\"\n        )\n\n\na boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced\n\n\nboostedTreeFit&lt;-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,\n               data = heartTrain,\n               method = \"gbm\",\n               preProcess=c(\"center\",\"scale\"),\n               trControl=trctrl,\n               tuneGrid=expand.grid(\n                 n.trees=c(25,50,100,200),\n                 interaction.depth=c(1,2,3),\n                 shrinkage=.01,\n                 n.minobsinnode=10),\n               verbose=FALSE\n                 )\n\nboostedPredict&lt;-predict(boostedTreeFit,\n        newdata = heartTest,\n        type=\"raw\"\n        )\nboostedPredict\n\n  [1] 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n [38] 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1\n [75] 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n[112] 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n[149] 0 0\nLevels: 0 1\n\n\nLastly, check how well each of your chosen models do on the test set using the confusionMatrix() function.\n\nconfusionMatrix(treePredict,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 62 42\n         1 12 34\n                                          \n               Accuracy : 0.64            \n                 95% CI : (0.5577, 0.7167)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.0006761       \n                                          \n                  Kappa : 0.2837          \n                                          \n Mcnemar's Test P-Value : 7.933e-05       \n                                          \n            Sensitivity : 0.8378          \n            Specificity : 0.4474          \n         Pos Pred Value : 0.5962          \n         Neg Pred Value : 0.7391          \n             Prevalence : 0.4933          \n         Detection Rate : 0.4133          \n   Detection Prevalence : 0.6933          \n      Balanced Accuracy : 0.6426          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(forrestPredict,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 55 34\n         1 19 42\n                                          \n               Accuracy : 0.6467          \n                 95% CI : (0.5645, 0.7229)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.000374        \n                                          \n                  Kappa : 0.2951          \n                                          \n Mcnemar's Test P-Value : 0.054474        \n                                          \n            Sensitivity : 0.7432          \n            Specificity : 0.5526          \n         Pos Pred Value : 0.6180          \n         Neg Pred Value : 0.6885          \n             Prevalence : 0.4933          \n         Detection Rate : 0.3667          \n   Detection Prevalence : 0.5933          \n      Balanced Accuracy : 0.6479          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(boostedPredict,heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 62 42\n         1 12 34\n                                          \n               Accuracy : 0.64            \n                 95% CI : (0.5577, 0.7167)\n    No Information Rate : 0.5067          \n    P-Value [Acc &gt; NIR] : 0.0006761       \n                                          \n                  Kappa : 0.2837          \n                                          \n Mcnemar's Test P-Value : 7.933e-05       \n                                          \n            Sensitivity : 0.8378          \n            Specificity : 0.4474          \n         Pos Pred Value : 0.5962          \n         Neg Pred Value : 0.7391          \n             Prevalence : 0.4933          \n         Detection Rate : 0.4133          \n   Detection Prevalence : 0.6933          \n      Balanced Accuracy : 0.6426          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nWrap up\nWhich model overall did the best job (in terms of accuracy) on the test set?\n\nOverall the forest prediction did slightly better than the tree model and boosted tree model. The forest prediction had a 64.67% accuracy rate compared to the 64% accuracy of the other models. That being said the difference between the models is extremely small."
  }
]