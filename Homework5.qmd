---
title: "Homework5"
author: "Andrew Harvey"
date: "16 July 2024"
format: html
editor: visual
---

## Task 1: Conceptual Questions

1. What is the purpose of using cross-validation when fitting a random forest model?

+ The purpose for cross-validation in when fitting random forest models is to ensure that response variable is not overfitting and allows us to more clearly see how the model fits with new data. This is important in random forests to provide even more accuracy. 

2. Describe the bagged tree algorithm.

<mark> + The bagged tree algorithm is a method in which we attempt to minimize variance in many fitted trees. The bagged tree algorithm takes in multiple fitted trees and aggregates them over a designated response variable in an attempt to find a prediction that holds for the accumulated trees. This increases accuracy and lowers variance. <mark>

3. What is meant by a general linear model?

<mark> + A general linear model has a continuous response variable and can be both categorical and continuous predictors. A general linear model also follows the formula $$Y = \beta_{1}X_{1}+\beta_{2}X_{2}+...+\beta_{n}X_{n}$$ <mark>

4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

<mark> + Adding interaction terms when fitting multiple linear regression models allows you to see how different variables affect each other. When interaction terms are not included in the model the model will focus on the response variable alone. <mark>

5. Why do we split our data into a training and test set?

<mark> + Splitting data into a training and test set allows us to see how well our model preforms on different data. Splitting the data lets us train our model on our training set then test the model on the test set were we know the outcome which lets us review the performance of the model. <mark>

## Task 2: Fitting Models

### Quick EDA/Data Preparation

1. Quickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.

```{r Load Libraries, warning=FALSE,message=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
```

<mark> + This chunk takes in the original data set and uses the na_if function to change assumed missing/NULL values into "NA". Since a resting blood pressure and cholestoral of 0 is not possible while living we can assume that 0 is a null or missing value. This chunk then drops all rows with na observations and assigns it to a new data frame. Finally, this chunk prints a summary of the new data frame. <mark>

```{r Load data and summarise}
originalData<-read.csv("https://www4.stat.ncsu.edu/~online/datasets/heart.csv")

print(summary(originalData))

# If RestingBP or Cholesterol = 0 then patient would be dead thus we can assume 0 is a
# missing/Null value Replaces ) with NA
originalData$RestingBP<-na_if(originalData$RestingBP,0)
originalData$Cholesterol<-na_if(originalData$Cholesterol,0)

# Find number of NA rows
sum(is.na(originalData))

# Create new dataframe with no na rows
heartDisease<-originalData|>
  drop_na()

# Create Summary table with respect to heart disease
print(summary(heartDisease))

```


2. Create a new variable that is a factor version of the HeartDisease variable (if needed, this depends on how you read in your data). Remove the ST_Slope variable and the original HeartDisease variable (if applicable).

<mark> + This chunk drops the ST_Slope variable.<mark>

```{r Factor HeartDisease}
# Change HeartDisease to factor, drop ST_Slope
heartDisease<-heartDisease|>
  select(-ST_Slope)
```

3. We’ll be doing a kNN model below to predict whether or not someone has heart disease. To use kNN we generally want to have all numeric predictors (although we could try to create our own loss function as an alternative). In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina ChestPainType, and RestingECG. 

Create dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.

<mark> + This chunk creates dummy variables for all categorical variables then uses predict to transform the data frame into all numeric data (even though there are categorical variables). <mark>

```{r Remove Categorical Variables}

# Use dummyVars to transform all categorical variables to numerics.
# Adds each unique text entry as its own variable where 0=False and 1=True.
dummies<-dummyVars(~.,data = heartDisease, sep = ".")

heartDisease2<-predict(dummies,newdata = heartDisease)|>
  as_tibble()
```

### Split your Data

split your data into a training and test set. (ideally you'd do this prior to the EDA so that info from the EDA doesn't bias what you do modeling-wise, but that isn't usually done.)

<mark> + This chunk sets a seed for random numbers and then splits our data into a training and test set. <mark>
```{r Split Data}
# Set the seed to enable reproducible 
set.seed(8)
# Create a numeric vector that will be used to select rows in heartDisease_2 for split
trainingVector<- sample(1:nrow(heartDisease2),size = nrow(heartDisease2)*.8)

# Subset heartDisease_2 into  training and test data sets
heartTrain<-heartDisease2[trainingVector,]|>
  mutate(HeartDisease=as.factor(HeartDisease))

heartTest<-heartDisease2[-trainingVector,]|>
  mutate(HeartDisease=as.factor(HeartDisease))
```

### kNN

Next, we’ll fit a kNN model. The article <h>here<h> gives a great example of selecting the number of neighbors to use with the caret package.

You don’t have to use all the variables from your dataset when fitting the model. However, you should only use numeric variables.

They use repeated 10 fold cross-validation. Although computationally intensive, doing repeated CV helps to give a more stable prediction of CV error. This is similar to how a mean is less variable than a single value. Since there is some inherent randomness in doing a CV computation, we can get an overall more stable result by averaging a few runs of the CV algorithm!

Train the kNN model. Use repeated 10 fold cross-validation, with the number of repeats being 3. You should also preprocess the data by centering and scaling. When fitting the model, set the tuneGrid so that you are considering values of k of 1, 2, 3, . . . , 40. (Note: From the help for the train() function it says: tuneGrid A data frame with possible tuning values. The columns are named the same as the tuning parameters. The name of the tuning parameter here is k.)
Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.

<mark> + This chunk creates the trcrl variable which we will use to cross validate all of our data when training. It also trains the knn model. Within the knn model we change the HeartDisease variable to a factor.We then plot the model to view the optimal number of neighbors. We then use the predict function with our model and the test data. The confusion matrix thenshows how well our model preformed. <mark>

```{r kNN}
trctrl<-trainControl(method = "repeatedcv", number = 10, repeats = 3)

knn_fit<-train(HeartDisease ~.,
               data = heartTrain,
               method = "knn",
               trControl=trctrl,
               preProcess=c("center","scale"),
               tuneLength=10)

knn_fit

plot(knn_fit)

testPred<-predict(knn_fit,newdata = heartTest)
testPred

confusionMatrix(testPred,heartTest$HeartDisease)
```

### Logistic Regression

Using your EDA, posit three different logistic regression models. Note: You don’t have to use the dummy columns you made here as the glm() function (and the caret implementation of it) can handle factor/ character variables as predictors.

Fit those models on the training set, using repeated CV as done above. You can preprocess the data or not, up to you. 

<mark> + The following 3 chunks fit 3 glm models on different variables to see how well of an indicator each is for heart disease. each chunk trains a model, summaries the model, plots it, predicts the test dataset then shows the results using a confusion matrix.  <mark>

```{r train and fit ageHeart}

age_glmFit<-train(HeartDisease~Age,
                  data=heartTrain,
                  method="glm",
                  trControl = trctrl,
                  preProcess=c("center","scale"),
                  tuneLength=10,
                  family="binomial")
age_glmFit
summary(age_glmFit)

heartTrain$HeartDisease<-as.numeric(as.character(heartTrain$HeartDisease))

ageHeart_sum<-heartTrain|>
  group_by(Age)|>
  summarise(probHeartDisease = mean(HeartDisease), n=n())

ggplot(ageHeart_sum,aes(x=Age,y=probHeartDisease))+
  geom_point(stat = "identity",aes(size = n))+
  stat_smooth(data = heartTrain,
              aes(x=Age, y=HeartDisease),
              method = "glm",
              method.args = list(family=
                                   "binomial"))+
  theme(axis.text.x = element_text(angle=45,
                                   vjust = 1,
                                   hjust = 1))

ageTestPred<-predict(age_glmFit,
                     newdata = heartTest,
                     type = "raw")
ageTestPred

confusionMatrix(ageTestPred,as.factor(heartTest$HeartDisease))
```

```{r train and fit cholHeart}

chol_glmFit<-train(as.factor(HeartDisease) ~ Cholesterol,
                data = heartTrain,
                method = "glm",
                trControl=trctrl,
                preProcess=c("center","scale"),
                tuneLength=10,
                family="binomial")


chol_glmFit
summary(chol_glmFit)

cholHeart_sum<-heartTrain|>
  group_by(Cholesterol)|>
  summarise(probHeartDisease = mean(HeartDisease), n=n())



ggplot(cholHeart_sum,aes(x=Cholesterol,y=probHeartDisease),size=n)+
  geom_point(stat = "identity",aes(size = n))+
  stat_smooth(data = heartTrain,
              aes(x=Cholesterol, y=HeartDisease),
              method = "glm",
              method.args = list(family=
                                   "binomial"))+
  theme(axis.text.x = element_text(angle=45,
                                   vjust = 1,
                                   hjust = 1))

cholTestPred<-predict(chol_glmFit,
                     newdata = heartTest,
                     type = "raw")
cholTestPred

confusionMatrix(cholTestPred,heartTest$HeartDisease)
```

```{r train and fit maxHRHeart}


#Cross Validation preformed in trControl
maxHR_glmFit<-train(as.factor(HeartDisease) ~MaxHR,
               data = heartTrain,
               method = "glm",
               trControl=trctrl,
               preProcess=c("center","scale"),
               tuneLength=10,
               family="binomial")

maxHR_glmFit
summary(maxHR_glmFit)

maxHRHeart_sum<-heartTrain|>
  group_by(MaxHR)|>
  summarise(probHeartDisease = mean(HeartDisease), n=n())



ggplot(maxHRHeart_sum,aes(x=MaxHR,y=probHeartDisease),size=n)+
  geom_point(stat = "identity",aes(size = n))+
  stat_smooth(data = heartTrain,
              aes(x=MaxHR, y=HeartDisease),
              method = "glm",
              method.args = list(family=
                                   "binomial"))+
  theme(axis.text.x = element_text(angle=45,
                                   vjust = 1,
                                   hjust = 1))

maxHRTestPred<-predict(maxHR_glmFit,
                       newdata = heartTest,
                       type = "raw")
maxHRTestPred

confusionMatrix(maxHRTestPred,heartTest$HeartDisease)

```

Identify your best model and provide a basic summary of it.

<mark> + The best model is the Max Heart Rate by Probability of Heart Disease model. <mark>

```{r summary of MaxHR by Heart Disease model}
summary(maxHR_glmFit)
```

Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.

<mark> + This chunk uses a confusion martix to show how well the maxHR model preformed
```{r maxHR confusionMatrix}

confusionMatrix(maxHRTestPred,heartTest$HeartDisease)

```

### Tree Models

In this section we’ll fit a few different tree based models in a similar way as above!

Choose your own variables of interest (as with logistic regression, this models can accept factor/character variables as predictors). Use repeated 10 fold CV to select a best 

+ classification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002, . . . ,
0.1)

<mark> + This chunk preforms a classification tree model on three variables and predicts the outcome. <mark>

```{r clasification Tree}
#maxHR,RestingHR,cholesteral
maxHRFit<-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,
               data = heartTrain,
               method = "rpart",
               preProcess=c("center","scale"),
               trControl=trctrl,
               tuneGrid=data.frame(cp=c(seq(0,.1, by=0.001)
                                        )
                                   )
               )
print(maxHRFit)
plot(maxHRFit)

treePredict<-predict(maxHRFit,
        newdata = heartTest,
        type="raw"
        )
```

+ a random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , # of predictors (bagging is a special case here!)

<mark> + This chunk preforms a random forest model on three variables and predicts the outcome. <mark>

```{r random forest}
# Creates a random forrest to find out how many predictors should be used
ForrestFit<-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,
               data = heartTrain,
               method = "rf",
               preProcess=c("center","scale"),
               trControl=trctrl,
               tuneGrid=data.frame(mtry=1)
               )

forrestPredict<-predict(ForrestFit,
        newdata = heartTest,
        type="raw"
        )
```


+ a boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced

<mark> + This chunk preforms a boosted tree model on three variables and predicts the outcome. <mark>

```{r boosted tree}
boostedTreeFit<-train(as.factor(HeartDisease) ~MaxHR+Cholesterol+RestingBP,
               data = heartTrain,
               method = "gbm",
               preProcess=c("center","scale"),
               trControl=trctrl,
               tuneGrid=expand.grid(
                 n.trees=c(25,50,100,200),
                 interaction.depth=c(1,2,3),
                 shrinkage=.01,
                 n.minobsinnode=10),
               verbose=FALSE
                 )

boostedPredict<-predict(boostedTreeFit,
        newdata = heartTest,
        type="raw"
        )
boostedPredict
```


Lastly, check how well each of your chosen models do on the test set using the confusionMatrix() function.

<mark> + This chunk checks the preformace of all three tree models using the confuxion martix. <mark>

```{r Confusion Matrix Check}
print(confusionMatrix(treePredict,heartTest$HeartDisease))

print(confusionMatrix(forrestPredict,heartTest$HeartDisease))

print(confusionMatrix(boostedPredict,heartTest$HeartDisease))
```


### Wrap up
Which model overall did the best job (in terms of accuracy) on the test set?

<mark> + Overall the forest prediction did slightly better than the tree model and boosted tree model. The forest prediction had a 64.67% accuracy rate compared to the 64% accuracy of the other models. That being said the difference between the models is extremely small. <mark>